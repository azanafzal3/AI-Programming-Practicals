{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5e9aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION FROM SCRATCH\n",
      "================================\n",
      "\n",
      "1. Loading and preprocessing data...\n",
      "Successfully loaded data with 24854 rows and 6 columns\n",
      "First 5 rows:\n",
      "  Date_reported Sever_Location  Packet_Size  No._of_Packets  Attack_Packets  \\\n",
      "0     3/29/2020           Asia         67.0              91             2.0   \n",
      "1      4/5/2020           Asia        183.0             274             3.0   \n",
      "2     4/12/2020           Asia        247.0             521            10.0   \n",
      "3     4/19/2020           Asia        387.0             908            15.0   \n",
      "4     4/26/2020           Asia        422.0            1330            13.0   \n",
      "\n",
      "   Total_Loss(Y)  \n",
      "0              2  \n",
      "1              5  \n",
      "2             15  \n",
      "3             30  \n",
      "4             43  \n",
      "\n",
      "2. Preprocessing data...\n",
      "Features shape: (24854, 5)\n",
      "Target shape: (24854,)\n",
      "\n",
      "3. Converting to binary classification problem...\n",
      "Using threshold (median): 3962.5\n",
      "Class distribution: [12427 12427] (0s and 1s)\n",
      "\n",
      "4. Initializing parameters...\n",
      "Number of features: 5\n",
      "Initial theta: [0. 0. 0. 0. 0.]\n",
      "Learning rate: 0.01\n",
      "Number of iterations: 5000\n",
      "\n",
      "5. Running gradient descent...\n",
      "  Iteration 0: Cost = 0.692292\n",
      "  Iteration 500: Cost = 0.538525\n",
      "  Iteration 1000: Cost = 0.505844\n",
      "  Iteration 1500: Cost = 0.490026\n",
      "  Iteration 2000: Cost = 0.479467\n",
      "  Iteration 2500: Cost = 0.471346\n",
      "  Iteration 3000: Cost = 0.464648\n",
      "  Iteration 3500: Cost = 0.458902\n",
      "  Iteration 4000: Cost = 0.453849\n",
      "  Iteration 4500: Cost = 0.449330\n",
      "  Iteration 5000: Cost = 0.445237\n",
      "Final theta: [0.39578549 0.49542612 1.46297906 1.68714362 1.11699723]\n",
      "Final cost: 0.44523739891590136\n",
      "\n",
      "6. Evaluating model...\n",
      "  Accuracy: 79.37%\n",
      "  Precision: 0.8324\n",
      "  Recall: 0.7355\n",
      "  F1 Score: 0.7810\n",
      "\n",
      "  Confusion Matrix:\n",
      "  True Positives: 9140\n",
      "  False Positives: 1840\n",
      "  True Negatives: 10587\n",
      "  False Negatives: 3287\n",
      "\n",
      "7. Plotting results...\n",
      "  Cost history plot saved as 'cost_history.png'\n",
      "\n",
      "Logistic Regression completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire logistic regression pipeline\"\"\"\n",
    "    print(\"LOGISTIC REGRESSION FROM SCRATCH\")\n",
    "    print(\"================================\")\n",
    "    \n",
    "    # 1. Load and preprocess data\n",
    "    print(\"\\n1. Loading and preprocessing data...\")\n",
    "    try:\n",
    "        data = pd.read_csv('Attacks.csv')\n",
    "        print(f\"Successfully loaded data with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "        print(\"First 5 rows:\")\n",
    "        print(data.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Preprocess data\n",
    "    print(\"\\n2. Preprocessing data...\")\n",
    "    X, y = preprocess_data(data)\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    # 3. Convert to binary classification problem\n",
    "    print(\"\\n3. Converting to binary classification problem...\")\n",
    "    threshold = np.median(y)\n",
    "    y_binary = (y > threshold).astype(int)\n",
    "    print(f\"Using threshold (median): {threshold}\")\n",
    "    print(f\"Class distribution: {np.bincount(y_binary)} (0s and 1s)\")\n",
    "    \n",
    "    # 4. Initialize parameters\n",
    "    print(\"\\n4. Initializing parameters...\")\n",
    "    n_features = X.shape[1]\n",
    "    theta = np.zeros(n_features)\n",
    "    learning_rate = 0.01\n",
    "    num_iterations = 5000\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Initial theta: {theta}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Number of iterations: {num_iterations}\")\n",
    "    \n",
    "    # 5. Run gradient descent\n",
    "    print(\"\\n5. Running gradient descent...\")\n",
    "    theta, cost_history = gradient_descent(X, y_binary, theta, learning_rate, num_iterations)\n",
    "    print(f\"Final theta: {theta}\")\n",
    "    print(f\"Final cost: {cost_history[-1]}\")\n",
    "    \n",
    "    # 6. Evaluate model\n",
    "    print(\"\\n6. Evaluating model...\")\n",
    "    metrics = evaluate_model(X, y_binary, theta)\n",
    "    print_metrics(metrics)\n",
    "    \n",
    "    # 7. Plot results\n",
    "    print(\"\\n7. Plotting results...\")\n",
    "    plot_cost_history(cost_history)\n",
    "    \n",
    "    print(\"\\nLogistic Regression completed successfully!\")\n",
    "    return theta, X, y_binary, cost_history, metrics\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess the data for logistic regression\"\"\"\n",
    "    # Convert date to numerical feature\n",
    "    if 'Date_reported' in data.columns:\n",
    "        data['Date_reported'] = pd.to_datetime(data['Date_reported'])\n",
    "        data['Days_Since_Start'] = (data['Date_reported'] - data['Date_reported'].min()).dt.days\n",
    "        data = data.drop('Date_reported', axis=1)\n",
    "    \n",
    "    # Drop non-numeric columns except target\n",
    "    for col in data.columns:\n",
    "        if col != 'Total_Loss(Y)' and not np.issubdtype(data[col].dtype, np.number):\n",
    "            data = data.drop(col, axis=1)\n",
    "    \n",
    "    # Split features and target\n",
    "    X = data.drop('Total_Loss(Y)', axis=1)\n",
    "    y = data['Total_Loss(Y)'].values\n",
    "    \n",
    "    # Normalize features\n",
    "    X_normalized = (X - X.mean()) / X.std()\n",
    "    \n",
    "    # Add bias term\n",
    "    X_with_bias = np.column_stack((np.ones(X_normalized.shape[0]), X_normalized))\n",
    "    \n",
    "    return X_with_bias, y\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function with overflow protection\"\"\"\n",
    "    # Clip to avoid overflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"Cost function for logistic regression\"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    \n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    h = np.clip(h, epsilon, 1 - epsilon)\n",
    "    \n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, theta, learning_rate, num_iterations, print_every=500):\n",
    "    \"\"\"Gradient descent optimization algorithm\"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(num_iterations + 1):\n",
    "        # Calculate hypothesis\n",
    "        h = sigmoid(X @ theta)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        gradient = (1/m) * (X.T @ (h - y))\n",
    "        \n",
    "        # Update parameters\n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "        # Calculate and store cost\n",
    "        if i % print_every == 0:\n",
    "            cost = compute_cost(X, y, theta)\n",
    "            cost_history.append(cost)\n",
    "            print(f\"  Iteration {i}: Cost = {cost:.6f}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    \"\"\"Make predictions using the trained model\"\"\"\n",
    "    probabilities = sigmoid(X @ theta)\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    return predictions, probabilities\n",
    "\n",
    "def evaluate_model(X, y, theta):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    y_pred, y_prob = predict(X, theta)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    \n",
    "    # Confusion matrix components\n",
    "    true_pos = np.sum((y_pred == 1) & (y == 1))\n",
    "    false_pos = np.sum((y_pred == 1) & (y == 0))\n",
    "    true_neg = np.sum((y_pred == 0) & (y == 0))\n",
    "    false_neg = np.sum((y_pred == 0) & (y == 1))\n",
    "    \n",
    "    # Precision, recall, F1\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': {\n",
    "            'true_positive': true_pos,\n",
    "            'false_positive': false_pos,\n",
    "            'true_negative': true_neg,\n",
    "            'false_negative': false_neg\n",
    "        },\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print model performance metrics\"\"\"\n",
    "    print(f\"  Accuracy: {metrics['accuracy'] * 100:.2f}%\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(\"\\n  Confusion Matrix:\")\n",
    "    print(f\"  True Positives: {cm['true_positive']}\")\n",
    "    print(f\"  False Positives: {cm['false_positive']}\")\n",
    "    print(f\"  True Negatives: {cm['true_negative']}\")\n",
    "    print(f\"  False Negatives: {cm['false_negative']}\")\n",
    "\n",
    "def plot_cost_history(cost_history):\n",
    "    \"\"\"Plot cost history over iterations\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(0, len(cost_history) * 500, 500), cost_history)\n",
    "    plt.title('Cost Function Over Iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid(True)\n",
    "    try:\n",
    "        plt.savefig('cost_history.png')\n",
    "        print(\"  Cost history plot saved as 'cost_history.png'\")\n",
    "    except:\n",
    "        print(\"  Could not save plot. Displaying instead.\")\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
